{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MSDS686 Final Project (Distracted Drivers) Miglin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tmiglin/Distracted-Drivers/blob/main/MSDS686_Final_Project_(Distracted_Drivers)_Miglin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxBqGLGQ_kDh"
      },
      "source": [
        "# Statefarm Distracted Driver Detection \n",
        "According to the CDC motor vehicle safety divison, one in five car accidents are caused by a distracted driver. We have all heard this in many statefarm commericals but the question is if there is a way to classify a drivers behavior. Statefarm has sponsored a Kaggle competition to improve some worrying statistics provided by the CDC. This project will involve building a convulutional neural network using the functional API that will provide categorical image classification for distracted drivers. The network will build with categorical classification because I don't just want to know if the driver is distracted but what class of distraction they belong to (cell phone in hand, looking behind, hands off the wheel). Below I used Professor Sorauf's fit hack for convenience. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gml_xoheS5MF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4e7463dd-cde3-49b5-fd33-1348e0487ea1"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFRoKq6bbu9n"
      },
      "source": [
        "## Here I am just import the libraries necessary to complete my task\n",
        "#### *This is taken from the week 6 assignment since it proved to be what was necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8nJxNX0S5MX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model, model_to_dot # This will print model architecture.\n",
        "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Flatten, Activation # We add the Concatentate function\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, SeparableConv2D # We will use AveragePooling2D. Similar to MaxPooling but now we take the average value in the window.\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend\n",
        "from cv2 import imread\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display # Library to help view images\n",
        "from PIL import Image # Library to help view images\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Library for data augmentation\n",
        "import os, shutil # Library for navigating files\n",
        "from google.colab import drive # Library to mount google drives\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR6c1cpY09x2"
      },
      "source": [
        "### I downloaded the competition data to my google drive stream so I will be mounting the drive directory to this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMNjfHNhixaa"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmVmwSt1S5Mx"
      },
      "source": [
        "# Specify the base directory where images are located.  You need to save your data here.\n",
        "base_dir = '/content/gdrive/My Drive/Distracted_Driver/state-farm-distracted-driver-detection/imgs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgN88B6BJK-o"
      },
      "source": [
        "#### Before I explain the data it would be best to understand how the data will be split between train, test, and validation. The competition does not provide a validation data set, so instead I will be using a validation split to create a validation set for my models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHMgzJfvS5M8"
      },
      "source": [
        "# Specify the traning, validation, and test dirrectories.  \n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEVL_W_dNXQu"
      },
      "source": [
        "## The Data\n",
        "The data provided are images taken from dashboard cameras facing the driver. With the goal being to categorize what each driver is doing based on a certain amount of classes, the pictures can be of the same drivers doing different things. As seen in the below cells, the data set's provided can be separated into 10 different classes. Each class contains photos with attentive and non-attentive drivers, staying consistent to the class however. I've provided the labels for the class after taking a close look at an image from each c folder. I used these labels to fill in a dictionary of labels so that I could display each of the image classes below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YChCnqH7S5NB"
      },
      "source": [
        "classes = [folder for folder in os.listdir(train_dir)]\n",
        "\n",
        "# Description of picture class\n",
        "class_dict = {\n",
        "    'c0': 'safe driving',\n",
        "    'c1': 'texting - right',\n",
        "    'c2': 'talking on the phone - right',\n",
        "    'c3': 'texting - left',\n",
        "    'c4': 'talking on the phone - left',\n",
        "    'c5': 'operating the radio',\n",
        "    'c6': 'drinking',\n",
        "    'c7': 'reaching behind',\n",
        "    'c8': 'hair and makeup',\n",
        "    'c9': 'talking to passenger'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJfpOphmCLd"
      },
      "source": [
        "from cv2 import imread\n",
        "for item in classes:\n",
        "    path = os.path.join(train_dir,item)\n",
        "    count=0\n",
        "    for image in os.listdir(path):\n",
        "        image_array = imread(os.path.join(path,image))\n",
        "        count+=1\n",
        "        print(item)\n",
        "        print(class_dict[item])\n",
        "        plt.imshow(image_array)\n",
        "        plt.show()\n",
        "        print('\\n')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGP5W3ewUYBF"
      },
      "source": [
        "# Data Augmentation\n",
        "I am going to be using data augmentation to increase the diversity of the training data. This should improve the performance of my models. I won't be having too large of ranges to limit the time the model will take to train all of the data. I have 17,943 images to train the model on, 20% of those being used for validation. Then I have 4481 images to test my data on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9TefFy2S5Nk"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        shear_range=0.1,\n",
        "        width_shift_range = 0.1,\n",
        "        height_shift_range = 0.1,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training')\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation',\n",
        "        shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2uC-LNgX_B_"
      },
      "source": [
        "# The First Model\n",
        "\n",
        "This model is similar to the model used in week 5. I found this model architecture to be great for categorical image classfication after tooling around with it. I am normalizing after the convolution layers are merged together as well. The highest accuracy I saw from this model after 3 different runs was 93.4%, good accuracy but the training time was quite long.\n",
        "\n",
        "* I made a light change to the model by adding another dense layer and it really heightened the accuracy so I will stick with this model instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1oqUEMpvT2H"
      },
      "source": [
        "visible = Input(shape=(224,224,3))\n",
        "\n",
        "c1 = SeparableConv2D(64, (2,2), padding ='valid', activation = 'relu')(visible)\n",
        "\n",
        "c2 = SeparableConv2D(128, (2,2), padding = 'valid', activation ='relu')(c1)\n",
        "p1 = MaxPooling2D((2,2), padding = 'valid')(c2)\n",
        "\n",
        "c3 = SeparableConv2D(128, (1,1), padding = 'valid', activation ='relu')(c1)\n",
        "p2 = MaxPooling2D((2,2), padding = 'valid')(c3)\n",
        "\n",
        "merge1 = Concatenate(axis=-1)([p1,p2])\n",
        "\n",
        "norm1 = BatchNormalization()(merge1)\n",
        "\n",
        "c4 = SeparableConv2D(256, (2,2), padding = 'valid', activation = 'relu')(norm1)\n",
        "p3 = MaxPooling2D(2,2)(c4)\n",
        "\n",
        "flat = Flatten()(p3)\n",
        "\n",
        "hidden1 = Dense(512, activation = 'relu')(flat)\n",
        "drop1 = Dropout(0.2)(hidden1)\n",
        "\n",
        "hidden2 =Dense(256, activation = 'relu')(drop1)\n",
        "drop2 = Dropout(0.2)(hidden2)\n",
        "\n",
        "output = Dense(10, activation='softmax')(drop2)\n",
        "\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiHIPOO52jh1"
      },
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "               loss = 'categorical_crossentropy',\n",
        "               metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator,  \n",
        "          epochs = 100,  \n",
        "          validation_data=test_generator,\n",
        "          verbose = 1,\n",
        "          callbacks=[EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights = True)]\n",
        "          ,use_multiprocessing=True) \n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print('test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urm0LGOlX7bK"
      },
      "source": [
        "# The Second Model\n",
        "This model proved to be a success. This model follows a sort of AlexNet architecture with more hidden layers. Padding is consistent throughout the model as well as pooling. I stacked the convolutional layers with Normalization throughout as well. After 3 runs with this model the highest accuracy achieved sat at 93.7%, this model seems promising so now I will test it.\n",
        "\n",
        "* This model will no longer be tested as the accuracy has dropped"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGt70aIzmCqk"
      },
      "source": [
        "visible = Input(shape=(224,224,3))\n",
        "\n",
        "c1 = Conv2D(64, (2,2), padding ='valid', activation = 'relu')(visible)\n",
        "p1 = MaxPooling2D((2,2), padding = 'valid')(c1)\n",
        "\n",
        "c2 = Conv2D(128, (2,2), padding = 'valid', activation ='relu')(c1)\n",
        "p2 = MaxPooling2D((2,2), padding = 'valid')(c2)\n",
        "\n",
        "c3 = Conv2D(256, (2,2), padding = 'valid', activation = 'relu')(p2)\n",
        "p3 = MaxPooling2D((2,2), padding = 'valid')(c3)\n",
        "\n",
        "flat = Flatten()(p3)\n",
        "\n",
        "hidden1 = Dense(512, activation = 'relu')(flat)\n",
        "drop1 = Dropout(0.2)(hidden1)\n",
        "\n",
        "hidden2 = Dense(256, activation = 'relu')(drop1)\n",
        "drop2 = Dropout(0.2)(hidden2)\n",
        "\n",
        "output = Dense(10, activation='softmax')(drop2)\n",
        "\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKC1vlqPrm69"
      },
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "               loss = 'categorical_crossentropy',\n",
        "               metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator,  \n",
        "          epochs = 100,  \n",
        "          validation_data=test_generator,\n",
        "          verbose = 1,\n",
        "          callbacks=[EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights = True)]\n",
        "          ,use_multiprocessing=True) \n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print('test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Dx2bqpd5y4"
      },
      "source": [
        "model.predict(test_generator, use_multiprocessing=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xvQ0PUUcfHP"
      },
      "source": [
        "# Results Analysis\n",
        "This model is success with great predictions coming from it. The training time for the models have been a bit tasking, but for this project I am quite happy with the results. Below I have found a nice function framework for displaying images next to the predictions and I will include it below for some visual flare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t8jQDwYenMV"
      },
      "source": [
        "import cv2\n",
        "\n",
        "batch = test_generator.next()\n",
        "for i,j in zip(batch[0],batch[1]):\n",
        "    plt.imshow(i)\n",
        "    \n",
        "    plt.show()\n",
        "    print('Actual: ', class_dict['c'+str(np.argmax(j))])\n",
        "    #print('\\n')\n",
        "    #print('\\n')\n",
        "    #break\n",
        "\n",
        "    #image =cv2.resize(image,(224,224))\n",
        "    image=i.reshape(1,224,224,3)\n",
        "    print('predicted: ', class_dict['c'+ str(np.argmax(model.predict(image)))])\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}